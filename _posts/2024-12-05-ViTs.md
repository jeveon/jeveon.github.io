---
layout: post
title: "[논문 리뷰] Global vision transformer pruning with hessian-aware saliency"
categories: Paper
excerpt_image: https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png
tags: [ViTs, Network pruning]
---

Yang, H., Yin, H., Shen, M., Molchanov, P., Li, H., & Kautz, J. (2023). Global vision transformer pruning with hessian-aware saliency. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 18547-18557).
[논문 링크](https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Global_Vision_Transformer_Pruning_With_Hessian-Aware_Saliency_CVPR_2023_paper.html)

![ViT](https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png)

## Pruning Vision Transformers (ViTs)

I survey a study that applies **network pruning** to **Vision Transformers (ViTs)**, one of the most widely used models today, and proposes a new model. This paper introduces a **structural pruning approach** for ViTs to **reduce computational cost and model size** while maintaining performance [Yang et al., 2023]. Unlike conventional ViT models that rely on **uniform design dimensions**, this research proposes **redistributing parameters globally** through **structural pruning** to optimize efficiency.

### **Structural Pruning of ViTs**
The proposed method reallocates and prunes independent **structural components** of the ViT architecture, including:

- **Embedding dimensions (EMB)**
- **Number of heads in multi-head self-attention (MSA) layers ($H$)**
- **Output dimensions of $Q$ and $K$ projections ($QK$)**
- **Input and projection dimensions of $V$ for each head**
- **Hidden dimensions of MLPs**

This approach integrates **head alignment** to ensure consistent dimensions across **$QK$ and $V$** in all heads, optimizing the overall **MSA latency**. By **reconstructing the weights** of **$Q$, $K$, $V$, and PROJ** projection layers, the method effectively achieves **head-specific pruning** while addressing dimensional mismatches. **Experimental results** demonstrate **up to a 0.3% improvement in accuracy** under the same latency constraints, highlighting the efficiency of the proposed pruning strategy.

### **Hessian-Based Importance Ranking**
To evaluate the importance of parameter groups for pruning, the study employs a **Hessian-based group importance ranking**. Inspired by recent research on the **loss surface geometry** of deep neural networks, the **Hessian matrix** of the loss function with respect to the **structural parameter groups** is utilized. The **importance score** $I_S$ is derived from the **squared sum of Hessian eigenvalues**:

\[
I_S := \mathbb{E}_z \left\| \frac{\nabla_{g_S} L(g_S + h z) \nabla_{g_S} L(g_S)}{h} \right\|^2, \quad z \sim \mathcal{N}(0, 1).
\]

where **$h$** is a small positive constant, **$g_S$** represents the gate variables for the **structural group** $S$, and **$L$** denotes the loss function. This criterion **quantifies the sensitivity** of the loss to **perturbations in $S$** and ensures **minimal loss degradation** during pruning.

The importance score **$I_S$** is computed using **Hessian-vector multiplication**, approximated by **finite differences**, to **identify and remove the least important structural groups**, ensuring efficient pruning. This approach, similar to **Taylor-based pruning** in CNNs, focuses on **pruning structural components** of ViTs rather than **individual weights**. The study also highlights the **limitations of magnitude-based pruning methods**, which lack a **global pruning criterion** and often lead to **overly aggressive or insufficient pruning** of ViT components.

### **Latency-Aware Pruning**
Pruning is further tailored to **reduce latency** by introducing **latency-aware regularization** into the importance score:

$$
I^L_S(W) = I_S(W) - \eta \cdot \left( \text{Lat}(W) - \text{Lat}(W \setminus S) \right),
$$

where **$\text{Lat}(\cdot)$** represents the **latency of the model**, and **$\eta$** is a **regularization coefficient**. By **penalizing importance scores** based on their **latency impact**, this approach accelerates the achievement of **latency targets** with **minimal accuracy loss**. **Lookup tables** are used to **estimate latency** for different **structural configurations** efficiently.

### **Training Objective with Distillation**
The training objective combines **pruning for importance ranking** with **fine-tuning for weight updates**. The **loss function** incorporates both **hard distillation from a CNN teacher** and **full-model distillation from the original pretrained model**:

$$
L = \alpha \cdot L_{\text{full}} + L_{\text{CNN}}
$$

where **$L_{\text{CNN}}$** is defined as:

$$
L_{\text{CNN}} = L_{\text{CE}}(\Psi(z_c^s), Y) + L_{\text{CE}}(\Psi(z_d^s), Y_{\text{CNN}})
$$
and **$L_{\text{full}}$** is given by:

$$
L_{\text{full}} = L_{\text{KL}}(\Psi(z_c^s / \tau), \Psi(z_c^t / \tau)) + L_{\text{KL}}(\Psi(z_d^s / \tau), \Psi(z_d^t / \tau))
$$


Here:
- **$\Psi(\cdot)$** denotes the **softmax function**.
- **$L_{\text{CE}}$** and **$L_{\text{KL}}$** are the **cross-entropy** and **KL divergence losses**, respectively.
- **$z_c$** and **$z_d$** represent **logits from class and distillation tokens**.
- **$\tau$** is the **distillation temperature**.

Combining these terms ensures that the **pruned model aligns closely** with both the **pretrained model** and the **teacher model**.

---

### **Key Contributions**
The key contributions of this study are summarized as follows:

1. **Hessian-Based Importance Ranking**  
   A novel **Hessian-based importance ranking method** is proposed, providing a **robust pruning metric** that outperforms **magnitude-based criteria** for ViTs.

2. **Latency-Aware Regularization**  
   A **latency-aware regularization** term is introduced to **guide pruning** toward achieving **latency constraints** with **higher accuracy**.

3. **Compatibility with Ampere GPU Sparsity**  
   The proposed **structured pruning** ensures **compatibility** with **Ampere GPU sparsity**, enabling **efficient hardware acceleration**.

4. **Comprehensive Distillation Training**  
   A **training objective** combining **CNN hard distillation** and **full-model distillation** is designed to **fine-tune pruned models effectively**.
