---
layout: post
title: "[논문 리뷰] AL-SAR: Active learning for skeleton-based action recognition"
categories: Paper
excerpt_image: /assets/images/2025-07-31-ALSAR/AL-SAR-20250731222246418.webp
tags: [Active Learning, Action Recognition]
---

Li, Jingyuan, Trung Le, and Eli Shlizerman. "AL-SAR: Active learning for skeleton-based action recognition." _IEEE Transactions on Neural Networks and Learning Systems_ (2023). https://ieeexplore.ieee.org/abstract/document/10219180

## 1. Introduction 

Spatiotemporal sequences로부터의 Action recognition은 action recognition of human movements, understanding subject interaction, robotic control과 같은 다양한 응용 분야에서 핵심 요소로 사용된다. 영상 기반과 달리 skeleton data 기반은 불필요한 정보를 걸러냄으로써 행동을 간결하게 표현한다.

skeleton 데이터로 행동을 인식하는 방법들은 fully supervised 방식을 기반 하고 있다. 하지만 많은 수의 주석 데이터가 필요하다. 또한 이러한 주석은 많은 시간과 전문지식이 필요하다. 

이러한 문제를 해결하기 위해서 cluster를 사용하는 unsupervised 방식이 제안되었다. 하지만 이러한 방법들도 클러스터를 행동과 매칭하기 위해서는 많은 라벨이 필요하다.

따라서 Few-shot learning과 semi-supervised 방식이 제안되었다. 하지만 Few-shot 방식이 사용하는 보조 행동 클래스(주석 data)가 항상 존재하지 않으며, 보조 행동 클래스와 새로운 클래스 간에 데이터 분포가 유사해야 해서 사용이 어렵다. 또한 Semi-supervised의 경우, 모든 sample이 모델에 동일한 기여를 하지 않는 다는 점을 간과하고 있어 최소한의 sample로 학습을 해야할 필요성이 존재한다.

이러한 관점에서 Active learning 알고리즘이 제안되었고 이는 이미지 데이터에서 널리 사용되고 있다. 하지만 기존의 AL 방법을 다른 데이터에 적용할 경우 성능이 random보다 좋지 않은 경우가 존재한다. 따라서 본 논문에서는 skeleton data 기반 AL 방식인 AL-SAR을 제안한다. 


## 2. Related Work

AL 방식은 일반적으로 1) sample synthesis; 2) stream-based selective sampling; 3) pool-based sampling 로 나눈다. 

sample synthesis는 데이터를 생성하는 방식이지만 품질이 낮으며, stream-based selective sampling은 한번에 하나의 샘플만 고려한다. 마지막으로 pool-based sampling은 각 단계에서 여러 샘플을 선택하고 이 방식을 가장 많이 사용한다.

논문에서 제안하는 방식도 pool-based sampling method이다. 


## 3. Methods

![a](/assets/images/2025-07-31-ALSAR/AL-SAR-20250731222246418.webp)

![a](/assets/images/2025-07-31-ALSAR/AL-SAR-20250731222235519.webp)

여러 시점에서 기록된 video frame을 사용할 경우 view-invariant 표현으로 변환해서 사용하였다. 또한 AL-SAR 시스템은 총 세 가지로 나눠진다. A) skeleton sequence의 feature 학습, B) margin 기반 지표의 안정적 계산을 위한 multi-Head 매커니즘, C) latent space의 위치 정보와 마진 기반 불확실성 지표를 통합해 샘플을 선택하는 전략

### A. Preliminary: Learning Meaningful Latent Representations

Encoder와 Decoder framework를 사용해서 latent 공간으로 데이터를 embedding한다. 이를 학습할 때는 재구성 손실을 사용한다.

### B. Multi-Head Mechanism

pool 기반 AL에서 불확실성 기반 전략은 label sample이 제한적일 경우 편향된 예측을 만들 수 있다. 분류기의 예측은 네트워크 초기화나 학습 전략에 영향을 받으며, 모델이 일부 샘플에 대해 과도하게 확신을 가질 수 있다. 따라서 분류기의 예측을 통해서 불확실성을 측정하는 것을 신뢰할 수 없다.

이를 해결하기 위해서 multi-head 매커니즘을 설계해서 견고한 불확실성을 측정하도록 한다. 제안된 multi-head는 무작위로 초기화된 FC layer이며, 학습 과정동안 고정되어 있다. 이는 최종 분류기 직전에 병렬로 삽입된다.

학습 시에 분류기는 연결된 어떤 헤드로부터도 올바른 예측을 하도록 훈련된다. (하나의 헤드만 무작위로 선택된다.) Sample을 선택할 때는 모든 head를 활성화하고, 이들로부터 얻은 예측의 평균을 사용해서 불확실성을 계산한다.

### C. Active Selection

Data의 선택 과정은 두 가지 시나리오로 나눠진다. 초기 선택에서는 cold-start 문제로 간주되어서 분류기를 사용하지 않고 latent space에 있는 embedding 값들을 clustering하여 각 cluster의 중심에 해당하는 sample을 주석 대상으로 선정한다. clustering기법은 K-means가 사용되었다. k를 선택하는 수식은 아래와 같다.

$$ k = \frac{1}{N_{\text{iter}}} \times \text{percentage} \times |X|$$

후속 선택에서는 multi-head structure로 계산된 MI를 결합해 사용한다. 각 Cluster K 내에서 MI가 가장 낮은 샘플을 선택한다.


## 4. Experiments and Results

UWA30과 NW-UCLA, NTU RGB+D 60을 사용하였다. 

실험에서는 3개의 bi-GRU cell을 사용해서 encoder를 구성하였으며, 각 방향별로 1024개의 hidden unit을 사용하였다. 양방향의 hidden unit을 concatenate하여 2048차원의 feature를 생성하며, Decoder는 hidden size 2048의 단일 GRU로 구성됩니다. 각 head는 2048 × 1024 fully connected layer로 구성됩니다. 분류기 또한 fully connected layer로 구성되며, head의 출력을 입력으로 받아 샘플의 클래스 확률을 예측한다.

Adam Optimizer를 사용하였으며, Learning rate는 $10^{-4}$로 설정되고 UWA3D 및 NW-UCLA에서는 10 epoch마다, NTU RGB+D에서는 3 epoch마다 0.95씩 감소한다.

비교군들은 다음과 같다.

U (Uniform Sampling), CS (Core Set), DIS (Discriminator-based Selection), AUG( consistency-based AL under augmentation), ASSL, MS2L, SC3D

아래는 실험 결과이다.

![a](/assets/images/2025-07-31-ALSAR/AL-SAR-20250731222111052.webp)

![a](/assets/images/2025-07-31-ALSAR/AL-SAR-20250731222123332.webp)

![a](/assets/images/2025-07-31-ALSAR/AL-SAR-20250731222137220.webp)

![a](/assets/images/2025-07-31-ALSAR/AL-SAR-20250731222149441.webp)

![a](/assets/images/2025-07-31-ALSAR/AL-SAR-20250731222159825.webp)

![a](/assets/images/2025-07-31-ALSAR/AL-SAR-20250731222209674.webp)
