---
layout: post
title: "[논문 리뷰] Revisiting Random Channel Pruning for Neural Network Compression"
categories: Paper
excerpt_image: /assets/images/Random.PNG
tags: [Network Pruning, Random Channel Pruning]
---

Li, Y., Adamczewski, K., Li, W., Gu, S., Timofte, R., & Van Gool, L. (2022). Revisiting random channel pruning for neural network compression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 191-201). [논문 링크](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Revisiting_Random_Channel_Pruning_for_Neural_Network_Compression_CVPR_2022_paper.html)

![random](/assets/images/Random.PNG)

### Random Channel Pruning

Among the many studies on network pruning, this paper focuses on establishing a fair benchmark for evaluating pruning methods. It specifically addresses methods for accelerating channel pruning. Despite the variety of existing channel pruning algorithms, the paper highlights a lack of fair and direct comparisons between these methods. Emphasizing the critical role of channel configuration alongside learned weights, the paper introduces a novel perspective: pruning algorithms should focus on finding the optimal channel configuration. To this end, **Random Channel Pruning** is proposed as a strong baseline method [Li et al., 2022]. In this approach, the number of channels to prune at each layer is chosen randomly, and those channels are removed. Through extensive experiments, this simple yet effective method demonstrates competitive performance compared to advanced pruning techniques.

The study's key findings can be summarized as follows:

- Recent advanced channel pruning algorithms showed no significant performance improvements over simpler L1 or L2 norm-based pruning criteria, highlighting that simplicity often suffices and that these norms remain reliable baselines.
- Random pruning, which does not depend on pretrained models, achieved similar or even superior performance compared to traditional methods like L1/L2 norm or gradient-based approaches. This challenges the conventional reliance on pretrained networks and demonstrates the versatility of random pruning.
- Random pruning achieved performance within **0.5%** of more sophisticated techniques such as network architecture optimization and width expansion. This indicates that pruning alone has inherent limitations tied to the upper-bound performance of the original network.
- The performance of pruned networks was found to depend heavily on the number of fine-tuning epochs, with longer fine-tuning significantly enhancing their performance. This underscores the importance of post-pruning optimization.

This paper categorizes random pruning into three types:  

1. **Full Random**: Channels are pruned without constraints at each layer.
2. **Constrained Random**: Channels are randomly selected within the layer but follow predefined pruning ratios.
3. **Random Channel Count Selection**: Pruning ratios for each layer are sampled randomly, and channels are pruned accordingly.

These approaches enable efficient sampling within the channel configuration space, facilitating the exploration of diverse combinations. Moreover, random pruning is versatile enough to be applied to both pretrained networks and networks trained from scratch. For the latter, sub-networks can be trained and evaluated in parallel during each mini-batch, enabling efficient performance evaluation.

Random channel pruning offers a simple yet effective benchmark for comparing channel pruning algorithms. It critiques the complexity of existing methods, highlighting the need for global network optimization advancements. Using a basic sampling strategy, random pruning achieves strong results and encourages more efficient sampling techniques.

---

### **Key Contributions**

The significant contributions of this paper are as follows:

1. **Introduction of Random Channel Pruning**  
   The paper introduces Random Channel Pruning as a robust baseline for channel pruning. This method demonstrates competitive performance with advanced techniques while maintaining simplicity, emphasizing the critical role of efficient channel configuration.

2. **Establishment of a Fair Benchmark**  
   By highlighting the lack of fair benchmarks for comparing pruning algorithms, the study provides a straightforward yet effective evaluation framework. It demonstrates that even simple criteria like L1/L2 norm-based pruning can rival more complex methods.

3. **Challenging Pretraining Dependency**  
   The paper shows that random pruning can achieve similar or superior performance without relying on pretrained models, challenging the conventional dependence on pretrained networks in pruning.

4. **Exploration of Channel Configuration Space**  
   Through random sampling strategies, the method effectively explores diverse channel configurations, revealing the potential of simpler approaches in achieving efficient pruning.

5. **Insights into Fine-tuning Importance**  
   The study underscores the critical role of fine-tuning epochs in optimizing pruned networks, offering practical guidance for improving post-pruning performance.
