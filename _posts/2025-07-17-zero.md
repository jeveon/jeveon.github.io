---
layout: post
title: "[논문 리뷰] Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment"
categories: Paper
excerpt_image: /assets/images/Zero-2025-07-17/무제-20250717140755195.webp
tags: [Zero-shot Action Recognition, Skeleton, Prototype Contrastive Learning]
---


Zhou, Kai, et al. "Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment." _arXiv preprint arXiv:2507.00566_ (2025). https://arxiv.org/abs/2507.00566

## Introduction

Skeleton-based action recognition은 RGB 기반 방법들 보다 외형 변화에 강인하고 개인정보 보호 측면에서도 이점을 제공해 Human-robot ineraction, healthcare, rehabilitation(재활), sports analysis에도 많이 사용이 된다.

완전 지도학습으로 skeleton 데이터를 학습하는 것은 주석과 라벨링에 높은 비용을 사용하고 이로 인해 확장성이 떨어진다는 제약이 존재한다. 따라서 라벨링 없이도 정확한 인식이 가능한 zero-shot Skeleton-based action recognition이 필요하다.

zero-shot Skeleton-based action recognition는 knowledge transfer를 통해서 본 적 없는 동작들을 인식하지만 아래와 같은 두 가지 이유로 인해서 도전적인 과제이다.

1. 유사한 동작 간 미묘한 차이를 정확하게 구분하기 위해 매우 높은 판별력을 가진 모델이 필요하다. 
2. 사람의 동작은 다양한 방식으로 나타나기 때문에 학습한 동작에서 새로운 동작으로 일반화하는 것이 어렵다.

따라서 기존의 zero-shot Skeleton-based action recognition에서는 사전학습된 text encoder를 활용해 일반화를 진행하려고 하였으며 이와 같은 방식은 아래에 그림과 같이 나타난다. 

![a](/assets/images/Zero-2025-07-17/무제-20250717140755195.webp)

이 그림 처럼 이전의 방식은 두 단계의 학습 프레임워크를 가진다. 첫 번째로 skeleton 데이터를 활용해 skeleton encoder를 학습하고 이 encoder와 사전 학습된 text encoder를 활용해 alignment module를 학습한다. 이렇게 되면 skeleton 데이터와 text (class name)이 semantic feature space로 mapping 된다. 테스트 단계에서는 unseen skeleton 데이터의 feature와 unseen class의 feature간의 유사도를 계산하고 이로 최종 예측을 한다.

이러한 방식의 문제는 아래와 같다.
1. 학습과정에서 판별력 부족: 데이터로 사전학습한 후 alignment module을 학습하기 때문에 feature extractor가 skeleton 데이터와 text 데이터의 내재적 상관관계를 쉽게 포착하지 못한다. 따라서 모듈간의 불일치가 발생할 수 있다.
2. 테스트 시 alignment bais: 학습된 class와 보지 못한 class간의 distrbutional differences로 인해서 skeleton feature와 text feature간의 정렬이 어긋 날 수 있다.

이러한 문제를 해결하기 위해서 논문에서 새로운 방식을 제시하였으며, 이는 후술 하겠다.


## Related Work

### Skeleton-based action recognition

RNN -> CNN -> GCN -> Transfomer 순으로 발전, 본 논문에서는 ST-GCN과 Shift-GCN을 SMIE의 설정에 따라서 사용.


## Proposed Method

본 논문에서 제시하는 방법론은 총 두 가지 구성 요소를 가진다.

### 1. End-to-End Cross-modal Contrastive Training

![b](/assets/images/Zero-2025-07-17/11.png)

우선 1번 2번 식과 같이 skeleton encoder와 text encoder를 통해서 feature를 추출한다. $\psi$는 text encoder와 차원을 맞추기 위한 projection layer이다. 또한 두 feature의 유사도는 3번과 4번 식처럼 양방향으로 진행되며, 이 방식은 하나의 클래스에 여러 샘플이 존재하기 때문에 InfoNCE loss를 사용하는 대신 Kullback-Leibler(KL) divergence loss를 사용했다. 


![1](/assets/images/Zero-2025-07-17/무제-20250717142350379.webp)
### 2. Action Description Generation

총 네가지 행동 설명 생성 방식을 썼으며, 예시는 아래와 같다. Class name을 제외한 방식은 모두 GPT-3를 이용하여 만들어졌다.
![2](/assets/images/Zero-2025-07-17/무제-20250717143138026.webp)

### 3. Prototype-guided Text Feature Alignment


물론입니다. 아래는 해당 영문 섹션의 자세한 한국어 번역입니다:

---

### C. Prototype-guided Text Feature Alignment

![c](/assets/images/Zero-2025-07-17/22.png)

6번 식을 통해서 각 테스트 데이터에 pseudo label을 얻는다 그 후 해당 class에 속하는 skeleton feature들을 통해서 각 미지 class 의 prototype feature를 생성한다. 직관적으로는 이렇게 구해진 $S_k$의 중심을 prototype feature로 사용하면 잘 정렬된다고 생각할 수 있지만 일부 pseudo label이 잘못 할당 되었을 수 있다. 

따라서 이 문제를 해결하기 위해서 예측 엔트로피 $H(z)$를 기반으로 필터링을 진행 신로도가 높은 샘플만을 사용해서 최종적으로 $Z_k$ 집합을 구성한다. 이를 통해서 10번식과 같이 prototype을 설정한다. 돟나 이를 활용해서 테스트 샘플의 최종 예측클래스를 11번 식을 통해서 재분류 한다. 



![3](/assets/images/Zero-2025-07-17/무제-20250717144337826.webp)



## Experiment

Setting 1: NTU-60: 55/5(Seen/Unseen), NTU-120: 110/10, PKU-MMD: 46/5
ST-GCN을 skeleton encoder로 Sentences-BERT를 text encoder

Setting 2: NTU-60: 55/5 및 48/12, NTU-120: 110/10 및 96/24
Shift-GCN을 skeleton encoder로 Sentences-BERT를 text encoder

skeleton 데이터 50프레임으로 정규화 ST-GCN의 output은 256차원, sentence-BERT는 768차원, 256차원을 768차원으로 투영하는 linear layer 사용.

아래는 setting에 따른 실험결과이다.

![4](/assets/images/Zero-2025-07-17/무제-20250717144928987.webp)

![5](/assets/images/Zero-2025-07-17/무제-20250717144944856.webp)


아래는 Ablation study 결과이다.

![6](/assets/images/Zero-2025-07-17/무제-20250717145006470.webp)

![7](/assets/images/Zero-2025-07-17/무제-20250717145213499.webp)

![8](/assets/images/Zero-2025-07-17/무제-20250717145309268.webp)


아래는 시각화 결과이다.

![9](/assets/images/Zero-2025-07-17/무제-20250717145421685.webp)

![10](/assets/images/Zero-2025-07-17/무제-20250717145435252.webp)
