---
layout: post
title: "[논문 리뷰] Active One-shot Learning"
categories: Paper
excerpt_image: /assets/images/ALRL-2025-07-21/Untitled-20250721104330212.webp
tags: [Active Learning, Reinforcement Learning]
---

Woodward, Mark, and Chelsea Finn. "Active one-shot learning." _arXiv preprint arXiv:1702.06559_ (2017). https://arxiv.org/abs/1702.06559 

## 1. Introduction

Active learning은 모델이 어떤 data에 label을 붙여야 할지 선택함으로써 작업에 필요한 양을 줄이는 접근 방식입니다. 이러한 AL은 암 분류, 악성 코드 탐지, 자율 주행 등의 분야에서 사용됩니다. 본 논문에서는 meta-learning과 reinforcement learning을 결합해서 active learning을 진행하는 방식을 제안한다. 


## 2. Related Work

기존의 방법들은 seen data들과 현재 unseen data 간의 유사도 측정, 혹은 label 예측에 대한 불확실성 척도 같은 heuristic한 방식에 의존했다. 


## 4. Task Methodology

![a](/assets/images/ALRL-2025-07-21/Untitled-20250721104330212.webp)
위의 이미지는 meta learning 방식에 대해서 설명한다. 위 그림에 대해서 더 설명하자면, model은 episode의 각 시점에 이미지 $x_t$를 입력 받아, 해당 이미지의 라벨을 요청하거나 예측할 수 있다. 이를 정하는 것은 $a_t$이며, 이는 $\hat{y}_t$(Predict label) 또는 label의 요청 여부를 나타내는 비트로 구성된 one-hot vector이다. 

요청을 선택한 경우 true label $y_t$는 다음 시점 $t+1$ 에서 $x_{t+1}$과 함께 $y_t$가 주어진다. Reward는 -0.05가 주어진다.
예측을 선택한 경우 다음 시점에서 $y_t$ 대신 zero vector가 관측값으로 주어진다. 예측을 통해서 label을 맞출 경우 reward는 +1, 그렇지 않을 경우 -1이 주어진다.

강화학습에 사용되는 모델은 LSTM이다.

## 6. Experiment

본 실험으로 강화 학습을 통해서 active learning을 학습할 수 있는지와 모델이 데이터를 설별할 때 불확실성에 기반해서 추론하고 있는지 확인하고자 한다.

Omniglot 데이터 셋을 활용하였으며, pixel 값은 0.0~1.0 사이로 정규화 하고 28x28 pixel로 리사이징 함. {0°, 90°, 180°, 270°} 중 하나의 회전 각도가 모든 샘플에 적용되었으며, 이미지는 784차원의 vector로 펼쳐져서 $x_t$가 된다. 각 step은 50개의 episode로 구성된다.

아래는 실험 결과이다. 
![a](/assets/images/ALRL-2025-07-21/Untitled-20250721110613946.webp)
![a](/assets/images/ALRL-2025-07-21/Untitled-20250721110741981.webp)

실험의 결과를 간단하게 요약하면 model은 의사 결정을 할 때 자신의 불확실성을 효과적으로 추론하는 법 또한 학습한다는 것을 알 수 있었다. 또한 강화학습을 통해서도 충분이 AL이 학습이 가능하며, reward 변경을 통해서 정확도와 lable requests 사이의 trade off를 효과적으로 조절할 수 있다.
