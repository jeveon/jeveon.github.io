---
layout: post
title: "[논문 리뷰] CoCoDiff; Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion"
categories: Paper
excerpt_image: /assets/images/CoCoDiff/1.webp
tags: [Diffusion Model, Skeleton, Action Recognition]
---

Zhao, Zhifu, et al. "CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion." _arXiv preprint arXiv:2504.21266_ (2025).
https://arxiv.org/abs/2504.21266


# Introduction

Human Action Recognition은 human-computer ineraction이나 가상환경 등 다양한 분야에서 활용된다. 특히 최근에는 depth sensor의 발전 및 배경에 구애 받지 않는 특성으로 인해 skeleton-based human action recognition이 주목 받고 있다. 하지만 이런 skeleton 데이터는 개인의 키, 체형, 움직임 습관 등으로 인해 Intra-class variation이 생긴다.

이러한 문제를 해결하기 위해서 기존 연구들은 data augmentation을 다양성을 확보하고자 하였지만 아래와 같은 문제가 존재한다.

1. 계산 비효율성: 증강된 sample에서 특징 추출 시 많은 자원 계산이 필요하다.
2. 의미 불일치: 증강된 sample은 의미를 왜곡할 수 있다. 따라서 의미적 불일치 발생

이러한 문제를 해결하기 위해서 본 논문에서는 Diffusion model을 사용해서 다양한 행동 특징을 생성하는 방식을 제안한다. 여기에 더불어 의미 인식이 가능한 text 기반 지도 전략을 통해서 Coarse test와 fine text를 사용하여서 의미 불일치를 해결하고자 하였다.


# COARSE-FINE TEXT CO-GUIDANCE DIFFUSION MODEL

![a](/assets/images/CoCoDiff/1.webp)

해당 사진은 논문에서 제안한 모델이다.

가장 우선적으로는 GCN을 활용한 Feature Extraction Network를 통해서 High-level의 행동 특징을 추출한다. 그리고 이를 latent diffusion 모델의 입력으로 사용한다.

아래 사진은 Diffusion model에 Text encoder를 어떻게 사용했는지와 Fine text, Coarse text의 예시이다.

![b](/assets/images/CoCoDiff/2.webp)

![c](/assets/images/CoCoDiff/3.webp)

위 사진을 보면 알 수 있듯 Diffusion model 양방향으로 이루어져 있으며 입력은 GCN에서 나온 feature($x_0$)를 사용한다. 정방향을 통해서 가우시안 노이즈를 추가하여 $x_t$ 를 만들고 역방향 과정을 통해서 $\hat{x_0}$를 생성한다.

 이때 Fine Text의 경우 Text encoder에 통과 시킨 후 이를 Denosing 과정의 각 단계에 추가되며, Coarse Text의 경우 마찬가지로 Text enocoder를 통과한 후 생성된 $\hat{x_0}$와 Contrastive loss를 구하는데 사용된다. 
 
 Text는 GPT-3.5를 통해서 생성되었으며, Fine text는 상세한 동작 설명, Coarse Text는 동작 label 및 동의어들로 구성되어 있다.

학습에서 사용되는 loss funtions은 아래와 같이 구성된다.

![g](/assets/images/CoCoDiff/8.png)

4번 수식은 Fine text 지도에 사용되는 수식이며, Denoising 과정에 시간 임베딩과 fine text 임베딩을 추가해서 계산이 이루어진다. 이를 최소화해 재구성이 잘 이루어지도록 한다.

5번 6번 수식은 bi-directional contrastive loss이며 Coarse text 지도에 사용된다. 또한 같은 batch 내에 동일한 label을 가진 데이터가 여러개 존재할 수 있어서 Cross-entropy 대신 KL divergence를 사용해서 대조 손실을 구한다.

8번 수식은 이렇게 데이터 증강이 아닌 학습과 관련된 loss로 예측된 클래스 확률 분포와 실제 one-hot 라벨 간의 교차 엔트로피 손실이다.

# Experiments

실험은 NTU RGB+D, NTU RGB+D 120, Kinetics-Skeleton이 사용되었으며 데이터를 나누는 기준은 논문에 명시되어 있다. 모든 데이터는 64 frames 이며, batch size는 64, 64, 128이다. 

아래는 Ablation study의 결과이다.
![d](/assets/images/CoCoDiff/4.webp)
![e](/assets/images/CoCoDiff/5.webp)
![f](/assets/images/CoCoDiff/6.webp)


아래는 최종 결과이다. 

![g](/assets/images/CoCoDiff/7.webp)
